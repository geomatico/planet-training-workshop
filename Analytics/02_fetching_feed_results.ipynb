{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Analytic Feed Results\n",
    "This notebook shows how to paginate through Planet Analytic Feed Results for an existing analytics [Subscription](https://developers.planet.com/docs/analytics/#subscriptions) to construct a combined [geojson](https://geojson.org/) feature collection that can be imported into geospatial analysis tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To use this notebook, you need to have the following:\n",
    "- A Planet account with access to the Analytics API\n",
    "- A Planet API Key\n",
    "- An Analytics Subscription ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if your Planet API Key is not set as an environment variable, you can paste it below\n",
    "API_KEY = os.environ.get('PL_API_KEY', 'YOUR API KEY HERE')\n",
    "# construct auth tuple for use in the requests library\n",
    "BASIC_AUTH = (API_KEY, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the base url for the Planet Analytic Feeds product\n",
    "See the [Analytics API Docs](https://developers.planet.com/docs/analytics/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.planet.com/analytics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "subscriptions_list_url = BASE_URL + 'subscriptions'\n",
    "resp = requests.get(subscriptions_list_url, auth=BASIC_AUTH)\n",
    "if resp.status_code == 200:\n",
    "    print('Yay, you can access the Analytics API')\n",
    "    subscriptions = resp.json()['data']\n",
    "    print('Available subscriptions:', len(subscriptions))\n",
    "else:\n",
    "    print('Something is wrong:', resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Analytics Subscription of Interest\n",
    "Below we will list your available subscription ids and some metadata in a dataframe and then select a subscription of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1000\n",
    "df = pd.DataFrame(subscriptions)\n",
    "df['start'] = pd.to_datetime(df['startTime']).dt.date\n",
    "df['end'] = pd.to_datetime(df['endTime']).dt.date\n",
    "df[['id', 'title', 'description', 'start', 'end']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a subscription from which to pull results, and replace the ID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example ID is for a subscription of ship detections in the Port of Oakland\n",
    "# You can replace this ID with your own subscription ID\n",
    "SUBSCRIPTION_ID = '89301f80-1948-4a1b-907c-edd794cc5600'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting subscription results\n",
    "In this section, we will make sure that we can get data from the subscription of interest by fetching the latest page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Construct the url for the subscription's results collection\n",
    "subscription_results_url = BASE_URL + 'collections/' + SUBSCRIPTION_ID + '/items'\n",
    "print(\"Request URL: {}\".format(subscription_results_url))\n",
    "\n",
    "# Get subscription results collection\n",
    "resp = requests.get(subscription_results_url, auth=BASIC_AUTH)\n",
    "if resp.status_code == 200:\n",
    "    print('Yay, you can access analytic feed results!')\n",
    "    subscription_results = resp.json()\n",
    "    # print(json.dumps(subscription_results, sort_keys=True, indent=4))\n",
    "else:\n",
    "    print('Something is wrong:', resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response json above will only include the most recent 250 detections by default. For subscriptions with many results, you can page through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subscription_results['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More results can be fetched by following the `next` link. Let's look at the links section of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_results['links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more results, we will want the link with a `rel` of `next`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_link(results_json):\n",
    "    \"\"\"Given a response json from one page of subscription results, get the url for the next page of results.\"\"\"\n",
    "    for link in results_json['links']:\n",
    "        if link['rel'] == 'next':\n",
    "            return link['href']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_link = get_next_link(subscription_results)\n",
    "print('next page url: ' + next_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this url, we can fetch the next page of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_results = requests.get(next_link, auth=BASIC_AUTH).json()\n",
    "print(json.dumps(next_results, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page of results comes as one feature collection. We can combine the features from different pages of results into one big feature collection. Below we will page through all results in the subscription but retrieving only the features (detections) _published_ over the last day.\n",
    "\n",
    "Results in the API are ordered by a `created` timestamp. This corresponds the time that the feature was _published_ to a Feed and does not necessarily match the `observed` timestamp in the feature's properties, which corresponds to when the source imagery for a feature was _collected_ and hence when the object was _detected_. Therefore, we will be using the property `observed` to filter for those vessel detections over the past 24h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first grab the latest published features\n",
    "results = sorted(subscription_results['features'],key=lambda r: r[\"properties\"][\"observed\"], reverse=True)\n",
    "latest_feature = results[0]\n",
    "creation_datestring = latest_feature['created']\n",
    "print('latest feature observed date:', creation_datestring)\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "# this date string can be parsed as a datetime and converted to a date\n",
    "latest_date = parse(creation_datestring).date()\n",
    "latest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter all the features detected since 1 day before the last detection\n",
    "# If you have a running subscription, the lates created_date will be the same date as today most of the times\n",
    "from datetime import timedelta\n",
    "min_date = latest_date - timedelta(days=1)\n",
    "print('Aggregate all detections from after this date:', min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_collection = {'type': 'FeatureCollection', 'features': []}\n",
    "next_link = subscription_results_url\n",
    "\n",
    "while next_link:\n",
    "    results = requests.get(next_link, auth=BASIC_AUTH).json()\n",
    "    next_features = sorted(results['features'],key=lambda r: r[\"properties\"][\"observed\"], reverse=True)\n",
    "    if next_features:\n",
    "        # Add only the filtered features\n",
    "        for f in next_features:\n",
    "            if parse(f['properties']['observed']).date() >= min_date:\n",
    "                feature_collection['features'].extend([f])\n",
    "        \n",
    "        # Rechek if there are still more features within the requested time period\n",
    "        latest_feature_creation = parse(next_features[0]['properties']['observed']).date()\n",
    "        earliest_feature_creation = parse(next_features[-1]['properties']['observed']).date()\n",
    "        if earliest_feature_creation >= min_date:\n",
    "            print('Fetched {} features fetched ({}, {})'.format(\n",
    "                len(next_features), earliest_feature_creation, latest_feature_creation))\n",
    "            next_link = get_next_link(results)\n",
    "        else:\n",
    "            next_link = False\n",
    "            \n",
    "    else:\n",
    "        next_link = None\n",
    "\n",
    "# print(json.dumps(next_features[0], indent=2))\n",
    "\n",
    "print('Total features: {}'.format(len(feature_collection['features'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results\n",
    "We can now save the combined geojson feature collection to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "os.makedirs('data', exist_ok=True)\n",
    "filename = 'data/collection_{}.geojson'.format(SUBSCRIPTION_ID)\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(feature_collection, file)\n",
    "\n",
    "FileLink(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the aggregated geojson file with the file link above, try importing the data into a geojson-compatible tool for visualization and exploration:\n",
    "- [geojson.io](http://geojson.io/)\n",
    "- [kepler gl](https://kepler.gl/demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved geojson file can also be used to make a geopandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gpd.read_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
